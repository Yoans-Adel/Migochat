# External library without type stubs - using pyright directives for proper type checking
# pyright: reportAttributeAccessIssue=false, reportUnknownMemberType=false, reportUnknownVariableType=false
import logging
from typing import Optional, Dict, Any, List
from config.settings import settings
from app.services.core.base_service import AIService as BaseAIService

logger = logging.getLogger(__name__)

gemini_available = False

# Runtime imports - google.generativeai doesn't have type stubs
try:
    import google.generativeai as genai
    from google.generativeai.types import HarmCategory, HarmBlockThreshold
    gemini_available = True
except ImportError as import_error:
    logger.info(f"Gemini package not available: {import_error}")
    # Define fallback types when package not available
    genai = None
    HarmCategory = None
    HarmBlockThreshold = None


class GeminiService(BaseAIService):
    """
    Advanced Gemini AI service with multimodal support
    Supports: Text, Images, Audio â†’ Text responses
    """

    def __init__(self) -> None:
        super().__init__()
        self.api_key = settings.GEMINI_API_KEY

        # Model configurations for different use cases
        self.models: Dict[str, Any] = {
            'multimodal': None,      # For images + audio + text
            'text_fast': None,       # For text-only (Gemma)
            'text_quality': None,    # For high-quality text
        }

        self._initialize_models()
        # Initialize the service
        self.initialize()

    def _initialize_models(self) -> None:
        """Initialize multiple models for different use cases"""
        if not gemini_available or genai is None:
            # Suppress warning on first initialization only
            if not hasattr(self, '_warning_shown'):
                logger.info("Gemini AI service not available - using fallback responses")
                self._warning_shown = True
            return

        # Type narrowing: assert genai is not None after availability check
        assert genai is not None, "genai should be available after gemini_available check"
        assert HarmCategory is not None, "HarmCategory should be available"
        assert HarmBlockThreshold is not None, "HarmBlockThreshold should be available"

        try:
            genai.configure(api_key=self.api_key)

            # Multimodal model (images + audio + text)
            try:
                self.models['multimodal'] = genai.GenerativeModel('gemini-2.5-flash')
                logger.info("âœ… Multimodal model initialized: gemini-2.5-flash")
            except Exception as e:
                logger.warning(f"Failed to initialize multimodal model: {e}")

            # Fast text-only model (Gemma - Ø£Ø³Ø±Ø¹ ÙˆØ£Ø±Ø®Øµ Ù„Ù„Ù†ØµÙˆØµ)
            try:
                self.models['text_fast'] = genai.GenerativeModel('gemma-3-27b-it')
                logger.info("âœ… Fast text model initialized: gemma-3-27b-it")
            except Exception:
                logger.warning("Gemma not available, using gemini-2.5-flash-lite as fallback")
                try:
                    self.models['text_fast'] = genai.GenerativeModel('gemini-2.5-flash-lite')
                    logger.info("âœ… Fast text fallback: gemini-2.5-flash-lite")
                except Exception as fallback_error:
                    logger.error(f"Failed to initialize fast text fallback: {fallback_error}")

            # High-quality text model
            try:
                self.models['text_quality'] = genai.GenerativeModel('gemini-2.5-pro')
                logger.info("âœ… Quality text model initialized: gemini-2.5-pro")
            except Exception:
                # Fallback to flash if pro not available
                self.models['text_quality'] = self.models['multimodal']

        except Exception as e:
            logger.error(f"Failed to initialize Gemini models: {e}")
            self.models = {'multimodal': None, 'text_fast': None, 'text_quality': None}

    def generate_response(
        self,
        message: str,
        context: Optional[Dict[str, Any]] = None,  # Changed from user_context
        media_files: Optional[List[Dict[str, Any]]] = None,
        use_quality_model: bool = False
    ) -> str:
        """
        Generate AI response using Gemini (supports multimodal inputs)

        Args:
            message: User message text
            context: Additional context about the user (renamed from user_context)
            media_files: List of media files [{'type': 'image|audio', 'data': bytes|path, 'mime_type': str}]
            use_quality_model: Use high-quality model for complex queries

        Returns:
            Generated text response
        """
        if not gemini_available:
            logger.warning("Gemini not available, using fallback response")
            return self._fallback_response(message)

        try:
            # Determine which model to use
            if media_files and len(media_files) > 0:
                # Has images/audio â†’ use multimodal model
                model = self.models['multimodal']
                model_name = "gemini-2.5-flash (multimodal)"

                if not model:
                    logger.error("Multimodal model not available")
                    return self._fallback_response(message)

                return self._generate_multimodal_response(message, context, media_files, model)

            else:
                # Text-only â†’ use fast Gemma model or quality model
                if use_quality_model and self.models['text_quality']:
                    model = self.models['text_quality']
                    model_name = "gemini-2.5-pro (quality)"
                elif self.models['text_fast']:
                    model = self.models['text_fast']
                    model_name = "gemma-3-27b-it (fast)"
                else:
                    model = self.models['multimodal']
                    model_name = "gemini-2.5-flash (fallback)"

                if not model:
                    return self._fallback_response(message)

                return self._generate_text_response(message, context, model, model_name)

        except Exception as e:
            logger.error(f"Error generating Gemini response: {e}")
            return self._fallback_response(message)

    def _generate_text_response(
        self,
        message: str,
        context: Optional[Dict[str, Any]],  # Changed from user_context
        model: Any,
        model_name: str
    ) -> str:
        """Generate text-only response"""
        try:
            # Build context for the AI
            ai_context = self._build_context(context)

            # Create prompt optimized for fashion retail
            prompt = f"""You are Bww-Assistant, a friendly AI shopping assistant for BWW Store - a leading fashion retailer in Egypt specializing in men's, women's, and kids' fashion.

User Context: {ai_context}
User Message: {message}

Guidelines:
â€¢ Match the user's language (Arabic/English) automatically
â€¢ Be conversational, helpful, and enthusiastic about fashion
â€¢ Use emojis naturally (1-2 per response) ğŸ›ï¸ ğŸ‘• ğŸ‘— ğŸ‘Ÿ
â€¢ For product inquiries: Offer to search our catalog
â€¢ For price/availability: Explain we have real-time search
â€¢ Build rapport - ask follow-up questions when relevant
â€¢ Keep responses concise (2-3 sentences max)
â€¢ Professional yet warm tone

Respond naturally:"""

            # Type narrowing for external library types
            assert HarmCategory is not None and HarmBlockThreshold is not None

            # Generate response
            response = model.generate_content(
                prompt,
                safety_settings={
                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
                    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
                    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
                }
            )

            if response and response.text:
                logger.info(f"âœ… Response generated using: {model_name}")
                return response.text.strip()
            else:
                logger.warning(f"Empty response from {model_name}")
                return self._fallback_response(message)

        except Exception as e:
            logger.error(f"Error in text generation: {e}")
            return self._fallback_response(message)

    def _generate_multimodal_response(
        self,
        message: str,
        context: Optional[Dict[str, Any]],  # Changed from user_context
        media_files: List[Dict[str, Any]],
        model: Any
    ) -> str:
        """Generate response with images/audio input"""
        try:
            # Build context
            ai_context = self._build_context(context)

            # Prepare content parts
            content_parts: List[Any] = []

            # Add media files
            for media in media_files:
                media_type = media.get('type', 'unknown')
                mime_type = media.get('mime_type', 'application/octet-stream')

                if media_type == 'image':
                    # Image input
                    if 'data' in media:
                        # Raw bytes
                        content_parts.append({
                            'mime_type': mime_type,
                            'data': media['data']
                        })
                    elif 'path' in media:
                        # File path
                        with open(media['path'], 'rb') as f:
                            content_parts.append({
                                'mime_type': mime_type,
                                'data': f.read()
                            })

                    logger.info(f"ğŸ“· Added image input ({mime_type})")

                elif media_type == 'audio':
                    # Audio input
                    if 'data' in media:
                        content_parts.append({
                            'mime_type': mime_type,
                            'data': media['data']
                        })
                    elif 'path' in media:
                        with open(media['path'], 'rb') as f:
                            content_parts.append({
                                'mime_type': mime_type,
                                'data': f.read()
                            })

                    logger.info(f"ğŸ¤ Added audio input ({mime_type})")

            # Add text prompt
            prompt = f"""You are Bww-Assistant analyzing media from a customer.

User Context: {ai_context}
User Message: {message}

Instructions:
â€¢ Analyze any images or audio provided
â€¢ For images: Describe the fashion items you see (style, color, type)
â€¢ For audio: Transcribe and respond to the spoken message
â€¢ Match the user's language (Arabic/English)
â€¢ Be helpful and conversational
â€¢ Use emojis naturally ğŸ›ï¸ ğŸ‘• ğŸ“· ğŸ¤
â€¢ Keep responses concise (2-3 sentences)

Respond naturally:"""

            content_parts.append(prompt)

            # Type narrowing for external library types
            assert HarmCategory is not None and HarmBlockThreshold is not None

            # Generate response
            response = model.generate_content(
                content_parts,
                safety_settings={
                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
                    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
                    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
                }
            )

            if response and response.text:
                logger.info(f"âœ… Multimodal response generated ({len(media_files)} media files)")
                return response.text.strip()
            else:
                logger.warning("Empty multimodal response")
                return "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø· Ø§Ù„Ù…Ø±Ø³Ù„Ø©. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰."

        except Exception as e:
            logger.error(f"Error in multimodal generation: {e}")
            return f"Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±Ø©/Ø§Ù„ØµÙˆØª: {str(e)}"

    def _build_context(self, user_context: Optional[Dict[str, Any]] = None) -> str:
        """Build context string for the AI"""
        if not user_context:
            return "No additional context available"

        context_parts: List[str] = []

        if user_context.get('lead_stage'):
            context_parts.append(f"Lead stage: {user_context['lead_stage']}")

        if user_context.get('customer_type'):
            context_parts.append(f"Customer type: {user_context['customer_type']}")

        if user_context.get('customer_label'):
            context_parts.append(f"Customer label: {user_context['customer_label']}")

        if user_context.get('message_count', 0) > 0:
            context_parts.append(f"Previous messages: {user_context['message_count']}")

        return "; ".join(context_parts) if context_parts else "New customer"

    def detect_intent(self, message: str) -> Dict[str, Any]:
        """
        Detect user intent from message

        Returns:
            {
                'intent': str,  # greeting, product_search, price_inquiry, etc.
                'confidence': float,  # 0.0 to 1.0
                'entities': dict  # extracted entities
            }
        """
        message_lower = message.lower()

        # Greeting intent
        if any(word in message_lower for word in ['Ù…Ø±Ø­Ø¨Ø§', 'Ù‡Ù„Ø§', 'Ø§Ù„Ø³Ù„Ø§Ù…', 'Ø£Ù‡Ù„Ø§', 'ØµØ¨Ø§Ø­', 'Ù…Ø³Ø§Ø¡', 'hello', 'hi', 'hey']):
            return {
                'intent': 'greeting',
                'confidence': 0.95,
                'entities': {}
            }

        # Product search intent
        if any(word in message_lower for word in ['Ù…Ù†ØªØ¬', 'ÙØ³ØªØ§Ù†', 'Ù‚Ù…ÙŠØµ', 'Ø­Ø°Ø§Ø¡', 'Ù…Ù„Ø§Ø¨Ø³', 'product', 'dress', 'shirt', 'shoes']):
            return {
                'intent': 'product_search',
                'confidence': 0.90,
                'entities': {'product_type': message}
            }

        # Price inquiry intent
        if any(word in message_lower for word in ['Ø³Ø¹Ø±', 'price', 'ÙƒØ§Ù…', 'ÙƒÙ…', 'how much', 'cost']):
            return {
                'intent': 'price_inquiry',
                'confidence': 0.92,
                'entities': {}
            }

        # Help request intent
        if any(word in message_lower for word in ['Ù…Ø³Ø§Ø¹Ø¯Ø©', 'Ø³Ø§Ø¹Ø¯', 'help', 'assist', 'support']):
            return {
                'intent': 'help_request',
                'confidence': 0.88,
                'entities': {}
            }

        # Default: general inquiry
        return {
            'intent': 'general_inquiry',
            'confidence': 0.70,
            'entities': {}
        }

    def generate_fallback_response(self, message: str) -> str:
        """Public method for generating fallback responses"""
        return self._fallback_response(message)

    def _fallback_response(self, message: str) -> str:
        """Fallback response when Gemini is not available"""
        message_lower = message.lower()

        # Arabic greetings
        if any(word in message_lower for word in ['Ù…Ø±Ø­Ø¨Ø§', 'Ù‡Ù„Ø§', 'Ø§Ù„Ø³Ù„Ø§Ù…', 'Ø£Ù‡Ù„Ø§', 'ØµØ¨Ø§Ø­', 'Ù…Ø³Ø§Ø¡']):
            return "Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹! ğŸ‘‹ Ø£Ù†Ø§ Ù…Ø³Ø§Ø¹Ø¯ BWW Store Ø§Ù„Ø°ÙƒÙŠ. ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ:\n\nğŸ›ï¸ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª\nğŸ’° Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø£Ø³Ø¹Ø§Ø±\nğŸ“¦ Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø± Ø¹Ù† Ø§Ù„ØªÙˆØ§ÙØ±\nğŸ“ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ¬Ø±\n\nÙ…Ø§Ø°Ø§ ØªØ±ÙŠØ¯ Ø§Ù„ÙŠÙˆÙ…ØŸ"

        # English greetings
        if any(word in message_lower for word in ['hello', 'hi', 'hey', 'good morning', 'good afternoon', 'greetings']):
            return "Hello! ğŸ‘‹ I'm BWW Store's smart assistant. I can help you with:\n\nğŸ›ï¸ Product search\nğŸ’° Prices\nğŸ“¦ Availability\nğŸ“ Store information\n\nWhat can I do for you today?"

        # Arabic help requests
        if any(word in message_lower for word in ['Ù…Ø³Ø§Ø¹Ø¯Ø©', 'Ø³Ø§Ø¹Ø¯', 'Ù…Ù…ÙƒÙ†', 'Ø¹Ø§ÙŠØ²', 'Ù…Ø­ØªØ§Ø¬']):
            return "Ø¨Ø§Ù„ØªØ£ÙƒÙŠØ¯! Ø³Ø£Ø³Ø§Ø¹Ø¯Ùƒ Ø¨ÙƒÙ„ Ø³Ø±ÙˆØ± ğŸ˜Š\n\nÙŠÙ…ÙƒÙ†Ù†ÙŠ:\nâœ¨ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£ÙŠ Ù…Ù†ØªØ¬ ÙÙŠ Ù…ØªØ¬Ø± BWW\nâœ¨ Ø¥Ø¹Ø·Ø§Ø¦Ùƒ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† Ø§Ù„Ø£Ø³Ø¹Ø§Ø± ÙˆØ§Ù„Ù…Ù‚Ø§Ø³Ø§Øª\nâœ¨ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…Ù†ØªØ¬ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨\n\nÙ…Ø§Ø°Ø§ ØªØ¨Ø­Ø« Ø¹Ù†Ù‡ Ø¨Ø§Ù„Ø¶Ø¨Ø·ØŸ"

        # English help requests
        if any(word in message_lower for word in ['help', 'assist', 'support', 'need', 'want']):
            return "Of course! I'd love to help! ğŸ˜Š\n\nI can:\nâœ¨ Search for any product in BWW Store\nâœ¨ Provide info about prices and sizes\nâœ¨ Help you choose the right product\n\nWhat exactly are you looking for?"

        # Arabic product requests
        if any(word in message_lower for word in ['Ù…Ù†ØªØ¬', 'ÙØ³ØªØ§Ù†', 'Ù‚Ù…ÙŠØµ', 'Ø­Ø°Ø§Ø¡', 'Ù…Ù„Ø§Ø¨Ø³', 'Ø¨Ù†Ø·Ù„ÙˆÙ†', 'Ø¬Ø§ÙƒÙŠØª']):
            return "Ø±Ø§Ø¦Ø¹! ğŸ‰ Ø¯Ø¹Ù†ÙŠ Ø£Ø³Ø§Ø¹Ø¯Ùƒ ÙÙŠ Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø§ ØªØ¨Ø­Ø« Ø¹Ù†Ù‡.\n\nØ£Ø®Ø¨Ø±Ù†ÙŠ Ø£ÙƒØ«Ø± Ø¹Ù†:\nğŸ“Œ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù†ØªØ¬\nğŸ“Œ Ø§Ù„Ù„ÙˆÙ† Ø§Ù„Ù…ÙØ¶Ù„\nğŸ“Œ Ø§Ù„Ù…Ù‚Ø§Ø³\nğŸ“Œ Ø§Ù„Ù…ÙŠØ²Ø§Ù†ÙŠØ©\n\nÙˆØ³Ø£Ø¬Ø¯ Ù„Ùƒ Ø£ÙØ¶Ù„ Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª! ğŸ›ï¸"

        # English product requests
        if any(word in message_lower for word in ['product', 'dress', 'shirt', 'shoes', 'clothes', 'fashion', 'pants', 'jacket']):
            return "Excellent! ğŸ‰ Let me help you find what you're looking for.\n\nTell me more about:\nğŸ“Œ Product type\nğŸ“Œ Preferred color\nğŸ“Œ Size\nğŸ“Œ Budget\n\nAnd I'll find you the best options! ğŸ›ï¸"

        # Price inquiries
        if any(word in message_lower for word in ['Ø³Ø¹Ø±', 'price', 'ÙƒØ§Ù…', 'ÙƒÙ…', 'how much', 'cost', 'ØªÙƒÙ„ÙØ©']):
            return "Ø£Ø³Ø¹Ø§Ø±Ù†Ø§ ØªÙ†Ø§ÙØ³ÙŠØ© Ø¬Ø¯Ø§Ù‹! ğŸ’°\n\nØ£Ø®Ø¨Ø±Ù†ÙŠ Ø¹Ù† Ø§Ù„Ù…Ù†ØªØ¬ Ø§Ù„Ù„ÙŠ Ø¹Ø§ÙŠØ² ØªØ¹Ø±Ù Ø³Ø¹Ø±Ù‡ØŒ ÙˆÙ‡Ø¯ÙŠÙƒ ÙƒÙ„ Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø¨Ù…Ø§ ÙÙŠÙ‡Ø§:\nâ€¢ Ø§Ù„Ø³Ø¹Ø± Ø§Ù„Ø­Ø§Ù„ÙŠ\nâ€¢ Ø£ÙŠ Ø¹Ø±ÙˆØ¶ Ù…ØªØ§Ø­Ø©\nâ€¢ Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„ØªÙˆØµÙŠÙ„"

        # Thanks
        if any(word in message_lower for word in ['Ø´ÙƒØ±Ø§', 'thank', 'thanks', 'thx']):
            return "Ø§Ù„Ø¹ÙÙˆ! ğŸŒŸ Ø£Ù†Ø§ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø§ÙŠÙ…Ø§Ù‹ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ. Ù„Ùˆ Ø§Ø­ØªØ¬Øª Ø£ÙŠ Ø­Ø§Ø¬Ø© ØªØ§Ù†ÙŠØ©ØŒ Ø§Ø¨Ø¹ØªÙ„ÙŠ!"

        # Default Arabic response
        if any(ord(char) >= 0x0600 and ord(char) <= 0x06FF for char in message):
            return "Ø£Ù†Ø§ Ù‡Ù†Ø§ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ! ğŸ˜Š\n\nÙŠÙ…ÙƒÙ†Ùƒ Ø£Ù† ØªØ³Ø£Ù„Ù†ÙŠ Ø¹Ù†:\nğŸ” Ù…Ù†ØªØ¬Ø§Øª Ù…Ø¹ÙŠÙ†Ø©\nğŸ’° Ø§Ù„Ø£Ø³Ø¹Ø§Ø±\nğŸ“¦ Ø§Ù„ØªÙˆØ§ÙØ±\nğŸšš Ø§Ù„ØªÙˆØµÙŠÙ„\n\nØ§ÙƒØªØ¨ Ù„ÙŠ Ø§Ù„Ù„ÙŠ Ù…Ø­ØªØ§Ø¬Ù‡ ÙˆØ£Ù†Ø§ Ù‡Ø³Ø§Ø¹Ø¯Ùƒ ÙÙˆØ±Ø§Ù‹!"

        # Default English response
        return "I'm here to help! ğŸ˜Š\n\nYou can ask me about:\nğŸ” Specific products\nğŸ’° Prices\nğŸ“¦ Availability\nğŸšš Delivery\n\nJust tell me what you need!"

    def is_available(self) -> bool:
        """Check if Gemini service is available"""
        return gemini_available and any(model is not None for model in self.models.values())

    def get_model_info(self) -> Dict[str, Any]:
        """Get information about available models"""
        return {
            "service": "Gemini Multi-Model",
            "models": {
                "multimodal": {
                    "name": "gemini-2.5-flash",
                    "available": self.models['multimodal'] is not None,
                    "supports": ["text", "images", "audio"],
                    "use_case": "Images + Audio + Text â†’ Text"
                },
                "text_fast": {
                    "name": "gemma-3-27b-it",
                    "available": self.models['text_fast'] is not None,
                    "supports": ["text"],
                    "use_case": "Fast text-only responses (Ø£Ø³Ø±Ø¹ ÙˆØ£Ø±Ø®Øµ)"
                },
                "text_quality": {
                    "name": "gemini-2.5-pro",
                    "available": self.models['text_quality'] is not None,
                    "supports": ["text"],
                    "use_case": "High-quality text responses"
                }
            },
            "api_key_configured": bool(self.api_key),
            "package_installed": gemini_available,
            "strategy": "Smart routing: text-only â†’ Gemma (fast), multimodal â†’ Gemini Flash"
        }

    def _do_initialize(self) -> bool:
        """Initialize Gemini service"""
        try:
            self._initialize_models()
            return True
        except Exception as e:
            logger.error(f"Failed to initialize Gemini service: {e}")
            return False

    def _generate_response_impl(self, query: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """AI-specific response generation"""
        try:
            # Extract media files if present in context
            media_files: Optional[List[Dict[str, Any]]] = context.get('media_files') if context else None
            use_quality: bool = context.get('use_quality_model', False) if context else False

            response = self.generate_response(
                message=query,
                context=context,
                media_files=media_files,
                use_quality_model=use_quality
            )

            # Determine which model was used
            model_used = "fallback"
            if media_files:
                model_used = "gemini-2.5-flash (multimodal)"
            elif use_quality and self.models['text_quality']:
                model_used = "gemini-2.5-pro"
            elif self.models['text_fast']:
                model_used = "gemma-3-27b-it"
            else:
                model_used = "gemini-2.5-flash"

            return {
                "response": response,
                "success": True,
                "model_used": model_used,
                "multimodal": bool(media_files)
            }
        except Exception as e:
            self.logger.error(f"Error generating Gemini response: {e}")
            return {
                "response": "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ùƒ. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.",
                "success": False,
                "error": str(e),
                "model_used": "fallback"
            }
